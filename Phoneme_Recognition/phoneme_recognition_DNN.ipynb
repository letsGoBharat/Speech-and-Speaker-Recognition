{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from proto import mfcc, mspec\n",
    "from proto2 import concatHMMs, log_multivariate_normal_density_diag, viterbi\n",
    "from prondict import prondict\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, utils, losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lab3_proto.py\n",
    "\n",
    "def words2phones(wordList, pronDict, addSilence=True, addShortPause=True):\n",
    "    \"\"\" word2phones: converts word level to phone level transcription adding silence\n",
    "\n",
    "    Args:\n",
    "       wordList: list of word symbols\n",
    "       pronDict: pronunciation dictionary. The keys correspond to words in wordList\n",
    "       addSilence: if True, add initial and final silence\n",
    "       addShortPause: if True, add short pause model \"sp\" at end of each word\n",
    "    Output:\n",
    "       list of phone symbols\n",
    "    \"\"\"\n",
    "    phone_list = []\n",
    "    if addSilence:\n",
    "        phone_list.append('sil')\n",
    "    for symbol in wordList:\n",
    "        for item in pronDict[symbol]:\n",
    "            phone_list.append(item)\n",
    "        phone_list.append('sp')\n",
    "    if addSilence:\n",
    "        phone_list.append('sil')\n",
    "    return list(phone_list)\n",
    "\n",
    "def forcedAlignment(lmfcc, phoneHMMs, phoneTrans):\n",
    "    \"\"\" forcedAlignmen: aligns a phonetic transcription at the state level\n",
    "\n",
    "    Args:\n",
    "       lmfcc: NxD array of MFCC feature vectors (N vectors of dimension D)\n",
    "              computed the same way as for the training of phoneHMMs\n",
    "       phoneHMMs: set of phonetic Gaussian HMM models\n",
    "       phoneTrans: list of phonetic symbols to be aligned including initial and\n",
    "                   final silence\n",
    "\n",
    "    Returns:\n",
    "       list of strings in the form phoneme_index specifying, for each time step\n",
    "       the state from phoneHMMs corresponding to the viterbi path.\n",
    "    \"\"\"\n",
    "\n",
    "def hmmLoop(hmmmodels, namelist=None):\n",
    "    \"\"\" Combines HMM models in a loop\n",
    "\n",
    "    Args:\n",
    "       hmmmodels: list of dictionaries with the following keys:\n",
    "           name: phonetic or word symbol corresponding to the model\n",
    "           startprob: M+1 array with priori probability of state\n",
    "           transmat: (M+1)x(M+1) transition matrix\n",
    "           means: MxD array of mean vectors\n",
    "           covars: MxD array of variances\n",
    "       namelist: list of model names that we want to combine, if None,\n",
    "                 all the models in hmmmodels are used\n",
    "\n",
    "    D is the dimension of the feature vectors\n",
    "    M is the number of emitting states in each HMM model (could be\n",
    "      different in each model)\n",
    "\n",
    "    Output\n",
    "       combinedhmm: dictionary with the same keys as the input but\n",
    "                    combined models\n",
    "       stateMap: map between states in combinedhmm and states in the\n",
    "                 input models.\n",
    "\n",
    "    Examples:\n",
    "       phoneLoop = hmmLoop(phoneHMMs)\n",
    "       wordLoop = hmmLoop(wordHMMs, ['o', 'z', '1', '2', '3'])\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lab3_tools.py\n",
    "def path2info(path):\n",
    "    \"\"\"\n",
    "    path2info: parses paths in the TIDIGIT format and extracts information\n",
    "               about the speaker and the utterance\n",
    "\n",
    "    Example:\n",
    "    path2info('tidigits/disc_4.1.1/tidigits/train/man/ae/z9z6531a.wav')\n",
    "    \"\"\"\n",
    "    rest, filename = os.path.split(path)\n",
    "    rest, speakerID = os.path.split(rest)\n",
    "    rest, gender = os.path.split(rest)\n",
    "    digits = filename[:-5]\n",
    "    repetition = filename[-5]\n",
    "    return gender, speakerID, digits, repetition\n",
    "\n",
    "def loadAudio(filename):\n",
    "    \"\"\"\n",
    "    loadAudio: loads audio data from file using pysndfile\n",
    "\n",
    "    Note that, by default pysndfile converts the samples into floating point\n",
    "    numbers and rescales them in the range [-1, 1]. This is avoided by specifying\n",
    "    the option dtype=np.int16 which keeps both the original data type and range\n",
    "    of values.\n",
    "    \"\"\"\n",
    "    return sf.read(filename, dtype='int16')\n",
    "\n",
    "def frames2trans(sequence, outfilename=None, timestep=0.01):\n",
    "    \"\"\"\n",
    "    Outputs a standard transcription given a frame-by-frame\n",
    "    list of strings.\n",
    "\n",
    "    Example (using functions from Lab 1 and Lab 2):\n",
    "    phones = ['sil', 'sil', 'sil', 'ow', 'ow', 'ow', 'ow', 'ow', 'sil', 'sil']\n",
    "    trans = frames2trans(phones, 'oa.lab')\n",
    "\n",
    "    Then you can use, for example wavesurfer to open the wav file and the transcription\n",
    "    \"\"\"\n",
    "    sym = sequence[0]\n",
    "    start = 0\n",
    "    end = 0\n",
    "    trans = ''\n",
    "    for t in range(len(sequence)):\n",
    "        if sequence[t] != sym:\n",
    "            trans = trans + str(start) + ' ' + str(end) + ' ' + sym + '\\n'\n",
    "            sym = sequence[t]\n",
    "            start = end\n",
    "        end = end + timestep\n",
    "    trans = trans + str(start) + ' ' + str(end) + ' ' + sym + '\\n'\n",
    "    if outfilename != None:\n",
    "        with open(outfilename, 'w') as f:\n",
    "            f.write(trans)\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('man', 'ae', 'z9z6531', 'a')\n(array([11, 13, 11, ...,  9,  9,  9], dtype=int16), 20000)\n61\n"
    }
   ],
   "source": [
    "print(path2info('tidigits/disc_4.1.1/tidigits/train/man/ae/z9z6531a.wav'))\n",
    "print(loadAudio('tidigits/disc_4.1.1/tidigits/train/man/ae/z9z6531a.wav'))\n",
    "\n",
    "phoneHMMs = np.load('lab2_models_all.npz', allow_pickle = True)['phoneHMMs'].item()\n",
    "phones = sorted(phoneHMMs.keys())\n",
    "nstates = {phone: phoneHMMs[phone]['means'].shape[0] for phone in phones}\n",
    "stateList = [ph + '_' + str(id) for ph in phones for id in range(nstates[ph])]\n",
    "print(len(stateList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using python file i/o\n",
    "# f = open('phoneHmms_states.txt', 'w')\n",
    "# for state in stateList:\n",
    "#     f.write(state)\n",
    "# f.close()\n",
    "\n",
    "#using pandas df\n",
    "df = pd.DataFrame(stateList)\n",
    "df.to_csv('phoneHmms_states.csv', header=False, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['z', '4', '3']\n['sil', 'z', 'iy', 'r', 'ow', 'sp', 'f', 'ao', 'r', 'sp', 'th', 'r', 'iy', 'sp', 'sil']\n\n['sil_0', 'sil_1', 'sil_2', 'z_0', 'z_1', 'z_2', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'sp_0', 'f_0', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_2', 'r_0', 'r_1', 'r_2', 'sp_0', 'th_0', 'th_1', 'th_2', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_1', 'iy_2', 'sp_0', 'sil_0', 'sil_1', 'sil_2']\n\n61\n['sil_0', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_1', 'sil_2', 'z_0', 'z_0', 'z_0', 'z_0', 'z_1', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'z_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'ow_0', 'ow_1', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'ow_2', 'f_0', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_1', 'f_2', 'ao_0', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_1', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'ao_2', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_0', 'th_1', 'th_1', 'th_1', 'th_2', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_0', 'r_1', 'r_2', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_0', 'iy_1', 'iy_1', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'iy_2', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0', 'sil_0']\n[39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 58, 58, 58, 58, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 21, 21, 21, 21, 21, 21, 21, 21, 22, 23, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 35, 30, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 33, 33, 33, 34, 35, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 48, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 35, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n0 0.01 sil_0\n0.01 0.19000000000000003 sil_1\n0.19000000000000003 0.20000000000000004 sil_2\n0.20000000000000004 0.24000000000000007 z_0\n0.24000000000000007 0.25000000000000006 z_1\n0.25000000000000006 0.36000000000000015 z_2\n0.36000000000000015 0.4400000000000002 iy_0\n0.4400000000000002 0.45000000000000023 iy_1\n0.45000000000000023 0.46000000000000024 iy_2\n0.46000000000000024 0.5600000000000003 r_0\n0.5600000000000003 0.5700000000000003 r_1\n0.5700000000000003 0.5800000000000003 r_2\n0.5800000000000003 0.5900000000000003 ow_0\n0.5900000000000003 0.6000000000000003 ow_1\n0.6000000000000003 0.6900000000000004 ow_2\n0.6900000000000004 0.7000000000000004 f_0\n0.7000000000000004 0.8100000000000005 f_1\n0.8100000000000005 0.8200000000000005 f_2\n0.8200000000000005 0.8300000000000005 ao_0\n0.8300000000000005 0.9700000000000006 ao_1\n0.9700000000000006 1.0800000000000007 ao_2\n1.0800000000000007 1.1100000000000008 r_0\n1.1100000000000008 1.1200000000000008 r_1\n1.1200000000000008 1.1300000000000008 r_2\n1.1300000000000008 1.2300000000000009 th_0\n1.2300000000000009 1.260000000000001 th_1\n1.260000000000001 1.270000000000001 th_2\n1.270000000000001 1.360000000000001 r_0\n1.360000000000001 1.370000000000001 r_1\n1.370000000000001 1.380000000000001 r_2\n1.380000000000001 1.480000000000001 iy_0\n1.480000000000001 1.500000000000001 iy_1\n1.500000000000001 1.5800000000000012 iy_2\n1.5800000000000012 1.7800000000000014 sil_0\n\n"
    }
   ],
   "source": [
    "#For 1 File - 4.1 - 4.2\n",
    "filename = 'tidigits/disc_4.1.1/tidigits/train/man/nw/z43a.wav'\n",
    "samples, samplingrate = loadAudio(filename)\n",
    "lmfcc = mfcc(samples)\n",
    "wordTrans = list(path2info(filename)[2])\n",
    "print(wordTrans)\n",
    "phoneTrans = words2phones(wordTrans, prondict)\n",
    "print(phoneTrans)\n",
    "print()\n",
    "utteranceHMM = concatHMMs(phoneHMMs, phoneTrans)\n",
    "#print(utteranceHMM)\n",
    "\n",
    "stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]\n",
    "print(stateTrans)\n",
    "print()\n",
    "\n",
    "\n",
    "viterbi_loglik, viterbi_states = viterbi(log_multivariate_normal_density_diag(lmfcc, utteranceHMM['means'],\n",
    "                      utteranceHMM['covars']), np.log(utteranceHMM['startprob']),\n",
    "                      np.log(utteranceHMM['transmat']), True)\n",
    "\n",
    "state_list = pd.read_csv('phoneHmms_states.csv', header = None)\n",
    "state_list = state_list[0].to_list()\n",
    "print(len(state_list))\n",
    "\n",
    "viterbiStateTrans = [stateTrans[state] for state in viterbi_states]\n",
    "print(viterbiStateTrans)\n",
    "\n",
    "targets = [state_list.index(state) for state in viterbiStateTrans]\n",
    "print(targets)\n",
    "\n",
    "\n",
    "trans = frames2trans(viterbiStateTrans, outfilename='z43a.lab') #wavesurfer not working on my machine..need to verify this part!\n",
    "print(trans)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all files - 4.3\n",
    "def feature_extraction(samples):\n",
    "    lmfcc = mfcc(samples)\n",
    "    mspec_all = mspec(samples)\n",
    "    return lmfcc, mspec_all\n",
    "\n",
    "def forced_alignment(filename, phone_HMMs, pron_dict):\n",
    "    wordTrans = list(path2info(filename)[2])\n",
    "    phoneTrans = words2phones(wordTrans, pron_dict)\n",
    "    utteranceHMM = concatHMMs(phone_HMMs, phoneTrans)\n",
    "    stateTrans = [phone + '_' + str(stateid) for phone in phoneTrans for stateid in range(nstates[phone])]\n",
    "    return utteranceHMM, stateTrans\n",
    "\n",
    "def create_train_data(train_fpath, phone_HMMs, pron_dict, state_list):\n",
    "    traindata = []\n",
    "    for root, dirs, files in os.walk(train_fpath):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                filename = os.path.join(root, file)\n",
    "                samples, samplingrate = loadAudio(filename)\n",
    "                lmfcc, mspec_all = feature_extraction(samples)\n",
    "                utteranceHMM, stateTrans = forced_alignment(filename, phone_HMMs, pron_dict)\n",
    "                _, viterbi_states = viterbi(log_multivariate_normal_density_diag(lmfcc, utteranceHMM['means'],\n",
    "                          utteranceHMM['covars']), np.log(utteranceHMM['startprob']),\n",
    "                          np.log(utteranceHMM['transmat']), True)\n",
    "                viterbiStateTrans = [stateTrans[state] for state in viterbi_states]\n",
    "                targets = [state_list.index(state) for state in viterbiStateTrans]\n",
    "                traindata.append({'filename': filename, 'lmfcc': lmfcc, 'mspec': mspec_all, 'targets': targets})\n",
    "    return traindata\n",
    "                \n",
    "def create_test_data(test_path, phone_HMMs, pron_dict, state_list):\n",
    "    testdata = []\n",
    "    for root, dirs, files in os.walk(test_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                filename = os.path.join(root, file)\n",
    "                samples, samplingrate = loadAudio(filename)\n",
    "                lmfcc, mspec_all = feature_extraction(samples)\n",
    "                utteranceHMM, stateTrans = forced_alignment(filename, phone_HMMs, pron_dict)\n",
    "                _, viterbi_states = viterbi(log_multivariate_normal_density_diag(lmfcc, utteranceHMM['means'],\n",
    "                          utteranceHMM['covars']), np.log(utteranceHMM['startprob']),\n",
    "                          np.log(utteranceHMM['transmat']), True)\n",
    "                viterbiStateTrans = [stateTrans[state] for state in viterbi_states]\n",
    "                targets = [state_list.index(state) for state in viterbiStateTrans]\n",
    "                testdata.append({'filename': filename, 'lmfcc': lmfcc, 'mspec': mspec_all, 'targets': targets})\n",
    "    return testdata\n",
    "\n",
    "def split_data(train_fpath):    \n",
    "    traindata = np.load(train_fpath, allow_pickle=True)\n",
    "\n",
    "    samples = len(traindata)\n",
    "    val_size = int(0.1 * samples)\n",
    "    print(val_size)\n",
    "    val_n_speakers = int(val_size / 77) #Why 77? 77 is the number of smaples per speaker on the dataset\n",
    "\n",
    "    if val_n_speakers % 2 != 0:\n",
    "        val_n_speakers += 1\n",
    "\n",
    "    val_size = val_n_speakers * 77\n",
    "    print(val_size)\n",
    "    samples_gender = int(val_size / 2)\n",
    "\n",
    "    valdata = [traindata[i] for i in range(0, samples_gender)]\n",
    "    valdata.extend([traindata[i] for i in range(4235, 4235 + samples_gender)])\n",
    "    trainingdata = [sample for sample in traindata if sample['filename'] not in [x['filename'] for x in valdata]]\n",
    "\n",
    "    np.save('trainingdata.npy', trainingdata)\n",
    "    np.save('valdata.npy', valdata)\n",
    "    return\n",
    "\n",
    "def dynamic_features(data_fpath, input_type):\n",
    "    data = np.load(data_fpath, allow_pickle=True)\n",
    "    for sample in data:\n",
    "        dfeature_list = []\n",
    "        i_max = len(sample[input_type]) - 1\n",
    "        for i, feature in enumerate(sample[input_type]):\n",
    "            dfeature = np.zeros((7, feature.shape[0]))\n",
    "\n",
    "            dfeature[0] = sample[input_type][np.abs(i - 3)]\n",
    "            dfeature[1] = sample[input_type][np.abs(i - 2)]\n",
    "            dfeature[2] = sample[input_type][np.abs(i - 1)]\n",
    "            dfeature[3] = sample[input_type][i]\n",
    "            dfeature[4] = sample[input_type][i_max - np.abs(i_max - (i + 1))]\n",
    "            dfeature[5] = sample[input_type][i_max - np.abs(i_max - (i + 2))]\n",
    "            dfeature[6] = sample[input_type][i_max - np.abs(i_max - (i + 3))]\n",
    "            dfeature_list.append(dfeature)\n",
    "        sample['features'] = np.array(dfeature_list)\n",
    "\n",
    "    s = data_fpath.split('/')\n",
    "    np.save('d' + s[-1], data)\n",
    "    return\n",
    "\n",
    "def non_dynamic_features(data_fpath, input_type):\n",
    "    data = np.load(data_fpath, allow_pickle=True)\n",
    "    for sample in data:\n",
    "        feature_list = [np.array(feature) for feature in sample[input_type]]\n",
    "        sample['features'] = np.array(feature_list)\n",
    "    \n",
    "    s = data_fpath.split('/')\n",
    "    np.save('nd' + s[-1], data)\n",
    "    return\n",
    "\n",
    "\n",
    "def feature_std(dtrainfpath, dvalfpath, dtestfpath):\n",
    "    paths = [dtrainfpath, dvalfpath, dtestfpath]\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for p in paths:\n",
    "        ddata = np.load(p, allow_pickle=True)\n",
    "\n",
    "        N = 0\n",
    "        D = np.prod(np.array(ddata[0]['features']).shape[1:3])\n",
    "        for sample in ddata:\n",
    "            N += sample['features'].shape[0]\n",
    "\n",
    "        X = np.zeros((N, D))\n",
    "        y = np.zeros((N, 1))\n",
    "        prev_idx = 0\n",
    "        for sample in ddata:\n",
    "            dynamic_features = np.array(sample['features'])\n",
    "            n = dynamic_features.shape[0]\n",
    "            X[prev_idx:prev_idx + n] = dynamic_features.reshape((n, D))\n",
    "            y[prev_idx:prev_idx + n, 0] = sample['targets']\n",
    "            prev_idx += n\n",
    "        Xs.append(X)\n",
    "        ys.append(y)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(Xs[0])\n",
    "    sets = ['train', 'val', 'test']\n",
    "    for i, s in enumerate(sets):\n",
    "        Xs[i] = scaler.transform(Xs[i])\n",
    "        Xs[i] = Xs[i].astype('float32')\n",
    "        print(Xs[i].shape)\n",
    "        np.save('X_' + s + '.npy', Xs[i])\n",
    "        np.save('y_' + s + '.npy', ys[i])\n",
    "    return    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d861cc420837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#np.save('testdata.npy', testdata)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'traindata.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#dynamic_features('trainingdata.npy', input_type)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#dynamic_features('valdata.npy', input_type)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-788125e4accc>\u001b[0m in \u001b[0;36msplit_data\u001b[0;34m(train_fpath)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtraindata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 453\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    723\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    724\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "#input_type = 'mspec'\n",
    "#feature_type = 'dynamic'\n",
    "#\n",
    "#train_path = 'tidigits/disc_4.1.1/tidigits/train'\n",
    "#traindata = create_train_data(train_path, phoneHMMs, prondict, state_list)\n",
    "#\n",
    "#test_path = 'tidigits/disc_4.2.1/tidigits/test'\n",
    "#testdata = create_test_data(test_path, phoneHMMs, prondict, state_list)\n",
    "#\n",
    "#np.save('traindata.npy', traindata)\n",
    "#np.save('testdata.npy', testdata)\n",
    "\n",
    "split_data('traindata.npy')\n",
    "#dynamic_features('trainingdata.npy', input_type)\n",
    "#dynamic_features('valdata.npy', input_type)\n",
    "#dynamic_features('testdata.npy', input_type)\n",
    "#print('HEY')\n",
    "#non_dynamic_features('trainingdata.npy', input_type)\n",
    "#non_dynamic_features('valdata.npy', input_type)\n",
    "#non_dynamic_features('testdata.npy', input_type)\n",
    "#feature_std('dtrainingdata.npy', 'dvaldata.npy', 'dtestdata.npy')\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make lables one hot\n",
    "\n",
    "X_train = np.load('X_train.npz', allow_pickle=True)['X']\n",
    "X_test = np.load('X_test.npz', allow_pickle=True)['X']\n",
    "X_val = np.load('X_val.npz', allow_pickle=True)['X']\n",
    "\n",
    "y_train = np.load('y_train.npz', allow_pickle=True)['y']\n",
    "y_test = np.load('y_test.npz', allow_pickle=True)['y']\n",
    "y_val = np.load('y_val.npz', allow_pickle=True)['y']\n",
    "\n",
    "Y_train = tf.keras.utils.to_categorical(y_train)\n",
    "Y_val = tf.keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN model\n",
    "nlayers = 4\n",
    "\n",
    "for i in range(nlayers):\n",
    "    print(nlayers + \"Layers\")\n",
    "    \"\"\" input_type = 'lmfcc'\n",
    "    if input_type == 'lmfcc':\n",
    "        f_count = 13\n",
    "    elif input_type == 'mspec':\n",
    "        f_count = 40 \"\"\"\n",
    "    output_classes = 61 #Number of states in state list\n",
    "    f_count = X_train.shape[1]\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, input_shape=(f_count,), activation = 'relu'))\n",
    "    for i in range(nlayers):\n",
    "        model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(output_classes, activation = 'softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    #Model training\n",
    "    model.fit(X_train, Y_train, epochs=10, validation_data = (X_val, Y_val), batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit0913dc1ad5d3449bb30dc83c1d2c45ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}